# -*- coding: utf-8 -*-
"""Untitled82.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HwtcOkkqSPxSh5NcoOkk4XHEWypK8I3F
"""

# Required: transformers, torch, sklearn, peft, pandas, tkinter
import tkinter as tk
from tkinter import filedialog
import pandas as pd
import random

# === Step 1: Pick the CSV file ===
def pick_file():
    root = tk.Tk()
    root.withdraw()  # Hide main window
    file_path = filedialog.askopenfilename(title="üìÅ Select your CSV file")
    return file_path

file = pick_file()
print("Selected file:", file)

# === Step 2: Load the CSV into a DataFrame ===
# Assumes your CSV has 'user' and 'bot' columns
df = pd.read_csv(file)
print("‚úÖ CSV loaded. Sample data:")
print(df.head())

# Optional: Shuffle the data for training
df = df.sample(frac=1).reset_index(drop=True)

# === Step 3: Combine user and bot messages ===
combined_texts = [
    f"user: {q.lower()}\nbot: {a.lower()} end"
    for q, a in zip(df['user'], df['bot'])
]

# (Optional) Show a few
print("\nüéØ Sample combined text:")
for i in range(3):
    print(combined_texts[i])

import random

# Shuffle before splitting
random.shuffle(combined_texts)

# Train/val split
train_texts, val_texts = train_test_split(combined_texts, test_size=0.1, random_state=42)

# Tokenize
train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')
val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors='pt')

# Function to mask everything before the bot's response
def mask_user_input(input_ids_batch, texts):
    labels = input_ids_batch.clone()
    for i, text in enumerate(texts):
        bot_index = text.find("bot:")  # lowercase now
        if bot_index == -1:
            continue
        tokens_before_bot = tokenizer(text[:bot_index], truncation=True)['input_ids']
        mask_len = len(tokens_before_bot)
        labels[i, :mask_len] = -100  # Ignore user tokens in loss
    return labels

# Custom dataset with proper label masking
class ConversationDataset(Dataset):
    def __init__(self, encodings, texts):
        self.input_ids = encodings['input_ids']
        self.attention_mask = encodings['attention_mask']
        self.labels = mask_user_input(self.input_ids, texts)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

# Create datasets
train_dataset = ConversationDataset(train_encodings, train_texts)
val_dataset = ConversationDataset(val_encodings, val_texts)

# Clean up memory
del df
gc.collect()



from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config

model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

config = GPT2Config.from_pretrained(model_name)
config.pad_token_id = tokenizer.pad_token_id  # Ensure padding is recognized

model = GPT2LMHeadModel.from_pretrained(model_name, config=config)

model.resize_token_embeddings(len(tokenizer))

# Optional for memory optimization
model.config.use_cache = False
model.gradient_checkpointing_enable()







from peft import get_peft_model, LoraConfig

from peft import get_peft_model, LoraConfig


lora_config = LoraConfig(
    r=16,
    lora_alpha=64,
    lora_dropout=0.2,
    bias='none',
    task_type="CAUSAL_LM",
    target_modules=["c_attn", "c_proj"]
)

model.resize_token_embeddings(len(tokenizer))


model = get_peft_model(model, lora_config)
model.print_trainable_parameters()



import torch
import transformers




import numpy as np
from torch.nn import CrossEntropyLoss
from torch.utils.data import DataLoader, RandomSampler
from transformers import (
    GPT2LMHeadModel,
    GPT2Tokenizer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    Trainer,
)

# Load tokenizer and set padding token
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Data collator for GPT-style training (not MLM)
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./gpt2_lora_model",
    per_device_train_batch_size=12,
    num_train_epochs=10,
    logging_dir='./logs',
    logging_steps=50,
    save_steps=500,
    save_total_limit=2,
    report_to="none",
    fp16=True,
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
    disable_tqdm=False,
    learning_rate=10e-4,
    weight_decay=0.01,
    warmup_steps=500,
    lr_scheduler_type='linear',
      # <-- Evaluate after every epoch
)

# Resize token embeddings
model.resize_token_embeddings(len(tokenizer))
model.config.pad_token_id = tokenizer.eos_token_id

# ‚úÖ Custom Trainer with shuffling
class CustomTrainer(Trainer):
    def get_train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            sampler=RandomSampler(self.train_dataset),  # Shuffle every epoch
            batch_size=self.args.train_batch_size,
            collate_fn=self.data_collator,
            num_workers=self.args.dataloader_num_workers,
            pin_memory=self.args.dataloader_pin_memory,
        )

# ‚úÖ Replace with your model and datasets before running this
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
      # <-- validation loss will be tracked
    data_collator=data_collator,
    tokenizer=tokenizer,
)

# Train the model
trainer.train()

# Save final model and tokenizer
model.save_pretrained("./gpt2_lora_model")
tokenizer.save_pretrained("./gpt2_lora_model")