# PRODIGY_GA_02
# ğŸ§  GPT-2 LoRA Chatbot Finetuner

This is a **plug-and-play Python script** to finetune GPT-2 using your own chatbot-style data (like user-bot messages) with **LoRA (Low-Rank Adaptation)** for fast and memory-efficient training. Built for internships, GenAI projects, or just flexing your AI skills ğŸ’»ğŸ”¥

---

## ğŸš€ Features

- ğŸ“ File picker to load your CSV (no hardcoded paths)
- ğŸ§  Label-masking before `bot:` â€” trains the model to only predict bot replies
- ğŸ”„ Custom train-validation split
- ğŸ§° LoRA via PEFT for efficient finetuning (smaller VRAM needed)
- ğŸ¤– GPT-2 based transformer model
- âœ… Final model + tokenizer saved in `./gpt2_lora_model`

---

## ğŸ—‚ï¸ Expected CSV Format

Your dataset must have two columns:

```csv
user,bot
hi,hello there!
how are you?,i'm doing great! what about you?
